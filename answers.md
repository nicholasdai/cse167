1. segment_and_tokenize takes the corpus input string and outputs a list of sentences that are in the form of lists of strings (essentially a 2d list). Each sentence that is created is itself a list where the first item is a <START> token, the last item an <END> token, the second to last item a period (assuming there are multiple sentences), and the rest of the items being the words in the sentence in order, if those words occur more than once in the corpus. If a word only appears once in the corpus, the string in the list is instead a <UNKNOWN> token.
2. Essentially, make_word_to_ix is giving a unique index to each unique word. Each time a new unique word / token appears, a new index is given it (through incrementation), and then added to a dictionary to store said values. We can trace the sample sents [['The','dog','barked'],['The','cat','barked']]. First we reach the word 'The', and store it with the first unique index, 0. The second word is 'dog' and it is given the incremented index, 1. Since 'barked', the third word, is also unique, it becomes 2. We reach 'The' again, which was already marked as index 0, so no new values are added to the dictionary. Then 'cat' which is unique becomes 3. Finall,y 'barked' is already marked as 2, so no new values are added. We then finally return the dictionary {'The': 0, 'dog': 1, 'barked': 2, 'cat': 3}
3. vectorize_sents is called for every single sentence, and for each word in each sentence, it creates a vector that is of the length of the number of unique indexes (from make_word_to_ix), and the vector is entirely 0s except for a 1 at the index of which the word/token belongs to. For instance, in the previous example with the word_to_ix dictionary {'The': 0, 'dog': 1, 'barked': 2, 'cat': 3}, the first word in the first sentence would create the vector [1, 0, 0, 0] (4 unique indexes, and 'The' has index 0), while the second would be [0, 1, 0, 0], and so on and so forth. Essentially it does this by looking at each word in each sentence, creating a vector for it set to all 0s, and then finding the word in the word_to_ix dictionary and setting the one index that corresponds to the word to a 1.
4. An embedding matrix is created that constantly learns and changes each epoch. The function embed_word retrieves each word's embedding by performing a matrix multiplication between the embedding matrix and the word vector itself.
5. Each current hidden state (the output of the function), is produced by the sigmoid (f(x) = 1/(1+e^-x)) of the sum of the matrix multiplication of the weight matrix for the embedding and the current word embedding itself, the matrix multiplication of the previous hidden state weight matrix for the embedding and the previous word embedding vector, and the bias vector.  
6. Essentially, single_layer_perceptron takes a perceptron weight matrix and matrix multiplies it with the current hidden state vector. It then takes the softmax function of said output, which essentially transforms the vector into a vector of probabilities (dim = 0 means  this is all done across the first dimension of the input tensor, though this is just a 1d tensor) The vector of probabilities is calculated by the following function, where $p_i$ is each probability of the ith vector value, and $s_i$ is the value (score) if the ith vector value, and n is the length of the vector.

\[
p_i = \frac{e^{s_i}}{\sum_{j=1}^{n} e^{s_j}}
\]

7. Because the elman network processes all the words sequentially, the hidden state that is updated only takes in the current word and the previous hidden state as variables, so future words cannot retroactively affect past words.
8. When the first word is processed, the model's hidden state is 0, but as the first word is converted into a new embedding, it is given a new hidden state which has a calculation dependent on the word embedding, weight matrices and the bias term. As the previous hidden state is 0, only the word embedding, the current weight matrix, and the bias term affect the new hidden state. However now that the first word is passed in, the previous hidden state is no longer 0. Thus, the representation of the third word and all future representations take in a more complicated formula that includes the previous word embeddings and the previous hidden states, the latter of which is no longer 0. So the first hidden state is computed based on the first word and for each additional word in the sentence, the hidden state is constantly updated, taking into account every previous input (including the first word).
9. When you run the code by calling train, the loss starts high due to random initial weights. But over time, the loss decreases as the RNN learns from all the previous data. The gradients continue to adjust the weights in order to make better predictions. This implies that as the model continues to learn and decreases the loss, the probability distribution that is learned by the model keeps on improving to more accurately predict future words.
10. After doing some testing, we found that changing the learning rate from 0.001 to 0.01 caused the model to improve at a faster rate. Increasing the learning rate to 0.05 did lead to bigger improvements early on but plateaued early on, indicating that the learning rate was too high. We also tried changing the hidden_state_dim from 20 to 50 and while there was a faster initial reduction in loss, there wasn't a significant difference as the later epochs converged slower (possibly due to overfitting). Same can be said with an increase in embedding_dim, there wasn't a very significant difference between the learning rates and simply led to a longer time to train.
11. The Elman network would not be able to model this dependency very well due of its limited memory. With the nature of the model, the Elman network struggles with longer range dependences, which can be explained by the vanishing gradient problem. The model continues to update and learn from each step meaning that over the span of a couple hundred words, the model struggles to remember important context from earlier in the sequence.
12. 
  1) 
     h_1 = σ(0.5) = 0.622  
     y_1 = softmax(0.0622, 0.1244 ,0.0622) = [0.326, 0.347, 0.326]  
     L_1 = -log(0.347) = 1.058  
     h_2 = σ(0.611) = 0.648  
     y_2 = softmax(0.0648,0.1296,0.0648) = [0.326, 0.347, 0.326]  
     L_2 = -log(0.326) = 1.121  
     L = 1.058 + 1.121 = 2.179  
  3) 
    ∂L/∂W_h = 
  4)  
    ∂L/∂W_h = 
