1. segment_and_tokenize takes the corpus input string and outputs a list of sentences that are in the form of lists of strings (essentially a 2d list). Each sentence that is created is itself a list where the first item is a <START> token, the last item an <END> token, the second to last item a period (assuming there are multiple sentences), and the rest of the items being the words in the sentence in order, if those words occur more than once in the corpus. If a word only appears once in the corpus, the string in the list is instead a <UNKNOWN> token.
2. Essentially, make_word_to_ix is giving a unique index to each unique word. Each time a new unique word / token appears, a new index is given it (through incrementation), and then added to a dictionary to store said values. We can trace the sample sents [['The','dog','barked'],['The','cat','barked']]. First we reach the word 'The', and store it with the first unique index, 0. The second word is 'dog' and it is given the incremented index, 1. Since 'barked', the third word, is also unique, it becomes 2. We reach 'The' again, which was already marked as index 0, so no new values are added to the dictionary. Then 'cat' which is unique becomes 3. Finall,y 'barked' is already marked as 2, so no new values are added. We then finally return the dictionary {'The': 0, 'dog': 1, 'barked': 2, 'cat': 3}
3. vectorize_sents is called for every single sentence, and for each word in each sentence, it creates a vector that is of the length of the number of unique indexes (from make_word_to_ix), and the vector is entirely 0s except for a 1 at the index of which the word/token belongs to. For instance, in the previous example with the word_to_ix dictionary {'The': 0, 'dog': 1, 'barked': 2, 'cat': 3}, the first word in the first sentence would create the vector [1, 0, 0, 0] (4 unique indexes, and 'The' has index 0), while the second would be [0, 1, 0, 0], and so on and so forth. Essentially it does this by looking at each word in each sentence, creating a vector for it set to all 0s, and then finding the word in the word_to_ix dictionary and setting the one index that corresponds to the word to a 1.
4. An embedding matrix is created that constantly learns and changes each epoch. The function embed_word retrieves each word's embedding by performing a matrix multiplication between the embedding matrix and the word vector itself.
5. Each current hidden state (the output of the function), is produced by the sigmoid (f(x) = 1/(1+e^-x)) of the sum of the matrix multiplication of the weight matrix for the embedding and the current word embedding itself, the matrix multiplication of the previous hidden state weight matrix for the embedding and the previous word embedding vector, and the bias vector. Essentially 
6. single_layer_perceptron takes a perceptron weight matrix and matrix multiplies it with the current hidden state vector. It then takes the softmax function of said output, which essentially transforms the vector into a vector of probabilities (dim = 0 means  this is all done across the first dimension of the input tensor, though this is just a 1d tensor) The vector of probabilities is calculated by the following function, where $p_i$ is each probability of the ith vector value, and $s_i$ is the value (score) if the ith vector value, and n is the length of the vector.

\[
p_i = \frac{e^{s_i}}{\sum_{j=1}^{n} e^{s_j}}
\]

7. Because the elman network processes all the words sequentially, the hidden state that is updated only takes in the current word and the previous hidden state as variables, so future words cannot retroactively affect past words.
8. When the first word is processed, the model's hidden state is 0, but as the first word is converted into a new embedding, it is given a new hidden state which has a calculation dependent on the word embedding, weight matrices and the bias term. As the previous hidden state is 0, only the word embedding, the current weight matrix, and the bias term affect the new hidden state. However now that the first word is passed in, the previous hidden state is no longer 0, so all future representations take in a more complicated formula that includes the previous word embeddings and the previous hidden states, the latter of which is no longer 0. 